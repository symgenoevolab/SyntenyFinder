{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bae23f-add4-4c26-919d-6bbcd1af3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isabel Jiah-Yih Liao\n",
    "# January 2024 \n",
    "# Synteny helper functions\n",
    "\n",
    "# This file contains the helper functions needed to run Synteny. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19febe1-f78b-439d-b1db-a6785a347b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synteny: \n",
    "    def __init__(self, root_directory, run_name, species_codes, proteome_ext = '.fa'): \n",
    "        self.run_name = run_name \n",
    "        self.dirs = dict() \n",
    "        self.dirs['root'] = root_directory \n",
    "        self.species_codes = species_codes \n",
    "        self.species_data = {species:dict() for species in species_codes}\n",
    "        self.check(proteome_ext)\n",
    "        self.build()\n",
    "\n",
    "    ###########\n",
    "    ### Initialising functions \n",
    "    ###########\n",
    "    # Check to see if all the necessary files are correctly named and can be found. \n",
    "    # Returns a dictionary dirs containing the appropriate file paths containing the input data\n",
    "    def check(self, proteome_ext): \n",
    "        self.add_dir('root', 'input_data') \n",
    "        for type, extension in [('proteomes', proteome_ext), \n",
    "                                ('genomes', '.fna'), \n",
    "                                ('gene_rows', '.gtf')]: \n",
    "            self.add_dir('input_data', type)\n",
    "\n",
    "            # print(f\"Checking files in {self.dirs[type]}...\")\n",
    "            contents = os.listdir(self.dirs[type])\n",
    "            contents = [filename for filename in contents if filename.endswith(extension)]\n",
    "            for species in species_codes: \n",
    "                files = [filename for filename in contents if filename.startswith(species)]\n",
    "                # print(files[0])\n",
    "                if len(files) != 1: \n",
    "                    print(f\"Incorrect number of {type} files found for {species} \\n{files}\")\n",
    "                else: \n",
    "                    identifier = f\"{species}_{type}\"\n",
    "                    self.dirs[identifier] = os.path.join(self.dirs[type], files[0])\n",
    "        print('File check complete') \n",
    "        return\n",
    "\n",
    "    # Builds neccesary directories to run\n",
    "    def build(self): \n",
    "        try: \n",
    "            self.make_dir('root', run_name)\n",
    "            self.make_dir(run_name, 'run_files')\n",
    "            self.make_dir(run_name, 'output')\n",
    "            self.make_dir('run_files', 'run_proteomes') \n",
    "            print('Directories built for run...') \n",
    "        except: \n",
    "            print('Adding existing directories...') \n",
    "            self.add_dir('root', run_name)\n",
    "            self.add_dir(run_name, 'run_files')\n",
    "            self.add_dir(run_name, 'output')\n",
    "            self.add_dir('run_files', 'run_proteomes') \n",
    "        try: \n",
    "            self.incorporate_orthofinder()\n",
    "            print('OrthoFinder results found at ' + self.dirs['orthofinder_results'])\n",
    "        except: \n",
    "            print('No orthofinder results found .')\n",
    "        self.dirs['save_synteny'] = os.path.join(self.dirs['run_files'], f\"{run_name}.pkl\") \n",
    "        return \n",
    "        \n",
    "    ###########\n",
    "    ### Preparing proteomes\n",
    "    ###########    \n",
    "    # Read proteome files and save the SeqRecords into a list. \n",
    "    # Return a dictionary sending each species code to its respective SeqRecords list. \n",
    "    def read_proteomes(self): \n",
    "        print('Reading proteomes...') \n",
    "        for species in self.species_codes: \n",
    "            filepath = self.dirs[f'{species}_proteomes']\n",
    "            records = []\n",
    "            with open(filepath, 'r') as handle: \n",
    "                for record in SeqIO.parse(handle, 'fasta'): \n",
    "                    records.append(record)\n",
    "            self.species_data[species]['proteomes'] = records\n",
    "        return self.species_data[species]['proteomes'][:10]\n",
    "\n",
    "    # For algs provided through adapted gene names in the proteome, parse the \n",
    "    # algs and simplify the gene names prior to running orthofinder. \n",
    "    def proteome_id_trim(self, species, position, delimiter = '_'): \n",
    "        index = position - 1 \n",
    "        \n",
    "        species_proteome = self.species_data[species]['proteomes']\n",
    "        gene_ids = {record.id : record.id.split(delimiter)[index] for record in species_proteome}\n",
    "       \n",
    "        for gene in self.species_data[species]['proteomes']: \n",
    "            gene.id = gene_ids[gene.id]\n",
    "            \n",
    "        return self.species_data[species]['proteomes'][:10]\n",
    "\n",
    "        \n",
    "    # Adds the three letter species code before each gene to make orthofinder output\n",
    "    # easer to understand. \n",
    "    # If no list of species codes is given, all proteomes are modified. \n",
    "    def proteome_add_species(self, species_list = None): \n",
    "        if species_list is None: \n",
    "            species_list = self.species_codes\n",
    "        try: \n",
    "            for species in species_list: \n",
    "                for gene in self.species_data[species]['proteomes']: \n",
    "                    gene.id = species + '|' + gene.id\n",
    "                    gene.description = ''\n",
    "                print('Proteome gene names modified for ' + species)\n",
    "            return self.species_data[species]['proteomes']\n",
    "        except: \n",
    "            print('Error occured in adding species codes to proteome.')\n",
    "            print('Check that read_proteomes() has been run on all species') \n",
    "            return \n",
    "\n",
    "    # Writes proteomes selected for use to run_proteomes folder in run_files\n",
    "    def write_proteomes(self): \n",
    "        try: \n",
    "            for species in self.species_codes: \n",
    "                new_proteome_file = os.path.join(self.dirs['run_proteomes'], f\"{species}.fa\")\n",
    "                self.dirs[f\"{species}_run_proteomes\"] = new_proteome_file \n",
    "                with open(new_proteome_file, 'w') as output_handle: \n",
    "                    SeqIO.write(self.species_data[species]['proteomes'], output_handle, 'fasta')\n",
    "                print(f'{species} proteomes successfully written to {new_proteome_file}.')\n",
    "            return self.species_data[species]['proteomes'][:10]\n",
    "        except: \n",
    "            print('Error occured in writing proteomes. Check that read_proteomes() has been run.') \n",
    "            return\n",
    "            \n",
    "    # Function to primary_transcript.py as specified in the OrthoFinder tutorial?\n",
    "\n",
    "    ###########\n",
    "    ### Orthofinder\n",
    "    ###########\n",
    "    # Running orthofinder using nohup \n",
    "    def run_orthofinder(self, orthofinder_path, threads):\n",
    "        self.dirs['orthofinder_executable'] = orthofinder_path\n",
    "        orthofinder_output = os.path.join(self.dirs['run_files'], 'orthofinder_output') \n",
    "        command = ['nohup', \n",
    "                   orthofinder_path, \n",
    "                   '-f', self.dirs['run_proteomes'], \n",
    "                   '-o', orthofinder_output, \n",
    "                   '-t', str(threads)]\n",
    "        print(command)\n",
    "        \n",
    "        result = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "        for line in result.stdout: \n",
    "            print(line.decode(), end = '')\n",
    "        result.wait()\n",
    "        return \n",
    "\n",
    "    # Incorporating Orthofinder results\n",
    "    def incorporate_orthofinder(self): \n",
    "        self.add_dir('run_files', 'orthofinder_output') \n",
    "        results_dir = os.listdir(self.dirs['orthofinder_output'])[-1]\n",
    "        self.dirs['orthofinder_results'] = os.path.join(self.dirs['orthofinder_output'], results_dir)\n",
    "        self.add_dir('orthofinder_results', 'Orthogroups')\n",
    "        self.dirs['orthogroups'] = os.path.join(self.dirs['Orthogroups'], 'Orthogroups.tsv')\n",
    "        self.orthogroups = pd.read_csv(self.dirs['orthogroups'], sep = '\\t') \n",
    "        self.dirs['sco'] = os.path.join(self.dirs['Orthogroups'], 'Orthogroups_SingleCopyOrthologues.txt')\n",
    "        return\n",
    "\n",
    "    # Return a table of single copy orthologues for a subset of species\n",
    "    # While it is possible to use the list of single copy orthologues generated by orthofinder, \n",
    "    # this method allows for more flexibility and will result in a larger number of shared genes \n",
    "    # if run with only a subset of the species used for orthofinder. \n",
    "    def single_copy_orthologues(self, species_list = None): \n",
    "        if species_list is None: \n",
    "            species_list = self.species_codes\n",
    "        orthogroup_subset = (self.orthogroups.copy()\n",
    "                                   [['Orthogroup'] + species_list].dropna())\n",
    "        multiple_copies = orthogroup_subset.map(lambda x: ',' in x) \n",
    "        self.single_copy_orthogroups  = orthogroup_subset[~multiple_copies.any(axis = 1)] \n",
    "        return self.single_copy_orthogroups\n",
    "\n",
    "    ###########\n",
    "    ### Building the karyotype file from genome\n",
    "    ###########   \n",
    "    def build_karyotype(self, chromosomes_per_species): \n",
    "        print('Reading karyotype files: this may take a moment...') \n",
    "        for species in self.species_codes: \n",
    "            species_karyotype = chromosomes_per_species[species]\n",
    "\n",
    "            # Using SeqIO to read the genome file, get the scaffold id and length for each entry. \n",
    "            genome_path = self.dirs[f'{species}_genomes']\n",
    "            genome_records = list(SeqIO.parse(genome_path, 'fasta'))\n",
    "            scaffolds = [{'Scaffold': record.id, 'Length': len(record.seq)} for record in genome_records] \n",
    "\n",
    "            # Sort the scaffolds from longest to shortest and get the top scaffolds\n",
    "            scaffolds = (pd.DataFrame(scaffolds).sort_values('Length', ascending = False)\n",
    "                                                .reset_index(drop = True))\n",
    "            chromosomes = scaffolds.head(species_karyotype) \n",
    "            chromosomes.columns = ['Chromosome', 'Length'] \n",
    "            self.species_data[species]['karyotype'] = chromosomes \n",
    "        return chromosomes\n",
    "\n",
    "    def clean_karyotype(self, columns, labels = None): \n",
    "        for species in self.species_codes: \n",
    "            karyotype = self.species_data[species]['karyotype']\n",
    "            cleaned_karyotype = pd.DataFrame(index = range(karyotype.shape[0]))\n",
    "            for header in columns: \n",
    "                if header in karyotype.columns: \n",
    "                    cleaned_karyotype[header] = karyotype[header]\n",
    "                elif header == 'SPECIES': \n",
    "                    cleaned_karyotype[header] = species\n",
    "                else: \n",
    "                    cleaned_karyotype[header] = header\n",
    "            if labels is not None: \n",
    "                cleaned_karyotype.columns = labels \n",
    "            self.species_data[species]['cleaned_karyotype'] = cleaned_karyotype \n",
    "        return cleaned_karyotype\n",
    "\n",
    "    def write_karyotype(self, index = False): \n",
    "        for species in self.species_codes: \n",
    "            file_path = os.path.join(self.dirs['output'], f\"{species}_karyotype.txt\")\n",
    "            self.species_data[species]['cleaned_karyotype'].to_csv(file_path, \n",
    "                                                                   sep = '\\t', \n",
    "                                                                   index = index) \n",
    "            \n",
    "    ###########\n",
    "    ### Incorporating gene positions from GTF\n",
    "    ###########\n",
    "    # Query to keep only rows with certain key words \n",
    "    def gtf_to_dataframe(self, species, annotation_type = None, feature = None, equivalence = ' '): \n",
    "        gtf_filepath = self.dirs[f'{species}_gene_rows']\n",
    "        columns = [\"sequence\", \"source\", \"feature\", \"start\", \"end\", \"score\", \"strand\", \"frame\", \"attribute\"]\n",
    "        gtf_dataframe = pd.read_csv(gtf_filepath, sep = '\\t', \n",
    "                                    comment = '#', \n",
    "                                    header = None, \n",
    "                                    names = columns, \n",
    "                                    dtype={'start': int, 'end': int})\n",
    "    \n",
    "        # Keep only rows of a given feature \n",
    "        if feature is not None: \n",
    "            gtf_dataframe = gtf_dataframe[gtf_dataframe.feature == feature]\n",
    "        \n",
    "        # Extract the specified annotation type from the 'attribute' column\n",
    "        if annotation_type is None: \n",
    "            gtf_dataframe['annotation'] = gtf_dataframe['attribute'] \n",
    "        else: \n",
    "            gtf_dataframe['annotation'] = (gtf_dataframe['attribute'].str\n",
    "                                          .extract(f'{annotation_type}{equivalence}\"?([^\";]+)\"?', expand=False))\n",
    "            gtf_dataframe = gtf_dataframe.dropna() \n",
    "        gtf_dataframe = gtf_dataframe.drop_duplicates(subset = 'annotation', keep = 'first') \n",
    "        self.species_data[species]['gene_rows'] = gtf_dataframe\n",
    "        return gtf_dataframe\n",
    "\n",
    "    # Add a species code with a pipe to the beginning of the chosen annotation names to match proteome gene name format \n",
    "    def gtf_add_species(self): \n",
    "        for species in self.species_codes: \n",
    "            all_files_read = True\n",
    "            if 'gene_rows' not in self.species_data[species]: \n",
    "                print(f'gtf file for {species} not yet read.') \n",
    "                all_files_read = False \n",
    "        if all_files_read == True: \n",
    "            for species in self.species_codes: \n",
    "                new_annotations = species + '|' + self.species_data[species]['gene_rows']['annotation']\n",
    "                self.species_data[species]['gene_rows']['annotation'] = new_annotations\n",
    "        return new_annotations\n",
    "\n",
    "    # Merges sco genes with the associated GTF information \n",
    "    def merge_gtf(self): \n",
    "        for species in self.species_codes: \n",
    "            gene_rows = self.species_data[species]['gene_rows'].copy()\n",
    "            orthogenes = self.single_copy_orthogroups[[species]].copy()\n",
    "            orthogene_coords = pd.merge(orthogenes, gene_rows, \n",
    "                               left_on = species, right_on = 'annotation', \n",
    "                               how = 'left')\n",
    "            self.species_data[species]['orthogene_coords'] = orthogene_coords\n",
    "        return orthogene_coords\n",
    "\n",
    "            \n",
    "    ###########\n",
    "    ### Incorporating lineage groups \n",
    "    ###########\n",
    "    # After merging the dataframes and creating the chromosome files, trace the genes based on \n",
    "    # their chromosome in a given species\n",
    "    def trace_chromosomes(self, species): \n",
    "        groups = self.species_data[species]['cleaned_karyotype']['Chr'].tolist()\n",
    "        print(groups)\n",
    "        group_ids = [chr(ord('A') + i) for i in range(len(groups))]\n",
    "        genes_to_groups = dict(zip(groups, group_ids))\n",
    "        genes = self.species_data[species]['orthogene_coords']['sequence']\n",
    "        gene_groups = [genes_to_groups[gene] if gene in genes_to_groups.keys() else gene for gene in genes ]\n",
    "        self.trace = gene_groups \n",
    "        return \n",
    "        \n",
    "    # For algs provided through adapted gene names in the proteome, parse the \n",
    "    # algs and simplify the gene names prior to running orthofinder. \n",
    "    def parse_algs(self, species, delimiter = '_', columns = ['index', 'alg', 'gene_id'], \n",
    "                                                               mapping_add_species = True): \n",
    "        try: \n",
    "            alg_index = columns.index('alg') \n",
    "        except: \n",
    "            raise Exception('alg column not specified') \n",
    "        try: \n",
    "            gene_id_index = columns.index('gene_id')\n",
    "        except: \n",
    "            raise Exception('gene_id column not specified') \n",
    "        \n",
    "        filepath = self.dirs[f'{species}_proteomes']\n",
    "        proteome_headers = []\n",
    "        with open(filepath, 'r') as handle: \n",
    "            for record in SeqIO.parse(handle, 'fasta'): \n",
    "                proteome_headers.append(record.id)\n",
    "        gene_headers = pd.DataFrame([record.split(delimiter) for record in proteome_headers]) \n",
    "        gene_headers.columns = columns\n",
    "        if mapping_add_species: \n",
    "            gene_headers['gene_id'] = species + '|' + gene_headers['gene_id']\n",
    "        self.alg_mapping = dict(zip(gene_headers['gene_id'], gene_headers['alg']))\n",
    "        return gene_headers\n",
    "        \n",
    "    def trace_algs(self, species): \n",
    "        if not hasattr(self, 'alg_mapping'): \n",
    "            raise Exception('No alg_mapping found. Please ensure parse_alg() has been run.')\n",
    "        genes = self.species_data[species]['orthogene_coords']['annotation'] \n",
    "        algs = [self.alg_mapping[gene] if gene in self.alg_mapping.keys() else gene for gene in genes ]\n",
    "        self.trace = algs \n",
    "        return\n",
    "\n",
    "    ###########\n",
    "    ### Format and write coordinate files\n",
    "    ###########  \n",
    "    # Reformat the merged coordinates dataframe. \n",
    "    def clean_coords(self, columns, labels = None): \n",
    "        for species in self.species_codes: \n",
    "            coords = self.species_data[species]['orthogene_coords'] \n",
    "            cleaned_coords = pd.DataFrame(index = range(coords.shape[0]))\n",
    "            for header in columns: \n",
    "                if header == \"TRACE\": \n",
    "                    cleaned_coords[header] = self.trace\n",
    "                elif header in coords.columns: \n",
    "                    cleaned_coords[header] = coords[header]\n",
    "                else: \n",
    "                    cleaned_coords[header] = header\n",
    "            self.species_data[species]['cleaned_coords'] = cleaned_coords\n",
    "            if labels is not None: \n",
    "                cleaned_coords.columns = labels\n",
    "        return cleaned_coords \n",
    "\n",
    "    # Write the cleaned coordinates to a csv file \n",
    "    def write_coords(self): \n",
    "        for species in self.species_codes: \n",
    "            file_path = os.path.join(self.dirs['output'], f\"{species}_coordinates.tsv\") \n",
    "            self.species_data[species]['cleaned_coords'].to_csv(file_path, \n",
    "                                                                sep = '\\t', \n",
    "                                                                header = False)\n",
    "            \n",
    "    ###########\n",
    "    ### Compatibility helpers\n",
    "    ###########\n",
    "    # Given a species code and a substring which indicates a suffix, truncate the gene names in \n",
    "    # the single_copy_orthogroups dataframe to omit the suffix. \n",
    "    def truncate_sco(self, species, suffix): \n",
    "        self.single_copy_orthogroups = (self.truncate(\n",
    "                                                 self.single_copy_orthogroups, \n",
    "                                                      species, suffix))\n",
    "        return self.single_copy_orthogroups\n",
    "        \n",
    "    # Given a species code and a substring which indicates a suffix, truncate the annotations in \n",
    "    # the gene_rows dataframe to omit the suffix. \n",
    "    def truncate_gene_rows(self, species, suffix):      \n",
    "        self.species_data[species]['gene_rows'] = (self.truncate(\n",
    "                                                                 self.species_data[species]['gene_rows'], \n",
    "                                                                     'annotation', suffix)) \n",
    "        return self.species_data[species]['gene_rows']\n",
    "\n",
    "        \n",
    "    # Save the object to a file so that the analysis can be continuted from a later point\n",
    "    def save_synteny(self): \n",
    "        with open(self.dirs['save_synteny'], 'wb') as file: \n",
    "            pickle.dump(self, file)\n",
    "\n",
    "    ###########\n",
    "    ### Helper functions \n",
    "    ###########\n",
    "    # Add a directory to dirs, assuming that the parent is already in dirs\n",
    "    def add_dir(self, parent, add): \n",
    "        new_path = os.path.join(self.dirs[parent], add)\n",
    "        if not os.path.exists(new_path): \n",
    "            raise Exception(f\"Expected directory at {new_path} was not found.\") \n",
    "            \n",
    "        else: \n",
    "            self.dirs[add] = new_path\n",
    "        return\n",
    "\n",
    "    # Create a new directory and add it to dirs\n",
    "    def make_dir(self, parent, make): \n",
    "        new_dir = os.path.join(self.dirs[parent], make)\n",
    "        os.makedirs(new_dir)\n",
    "        self.dirs[make] = new_dir\n",
    "        return \n",
    "\n",
    "    # Given a dataframe, a column, and a suffix, trim the suffix from the column entries and \n",
    "    # return the altered dataframe. \n",
    "    def truncate(self, dataframe, column, suffix): \n",
    "        dataframe = dataframe.copy()\n",
    "        entries_list = dataframe[column].tolist()\n",
    "        truncated = [self.truncate_gene(entry, suffix) for entry in entries_list] \n",
    "        dataframe[column] = truncated\n",
    "        return dataframe  \n",
    "        \n",
    "    def truncate_gene(self, gene, suffix): \n",
    "        try: \n",
    "            index = gene.find(suffix)\n",
    "            return(gene[:index] if index != -1 else gene)\n",
    "        except: \n",
    "            print('Issue detected in trucating gene.') \n",
    "            return(gene) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe470bc-c98b-4347-8782-730aebca4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synteny(root_directory, run_name): \n",
    "    saved_file = os.path.join(root_directory, run_name, 'run_files', f'{run_name}.pkl')\n",
    "    with open(saved_file, 'rb') as file: \n",
    "        synteny = pickle.load(file)\n",
    "    return(synteny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171a94c-99ad-4645-a2f0-7920e083a630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c2703-4ca6-4b2f-a0b2-2d237b2b82c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
